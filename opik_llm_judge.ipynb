{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Creating a Custom LLM Judge with Opik for Wais Wallet\n",
                "\n",
                "This notebook demonstrates how to create a custom LLM-as-a-judge evaluation system using Opik to evaluate the quality of Wais Wallet's AI Pilot responses.\n",
                "\n",
                "## What We'll Build\n",
                "- **Custom LLM Judge**: Evaluates responses on financial accuracy, tone, and helpfulness\n",
                "- **Automated Evaluation**: Tests AI responses against benchmark criterianl\n",
                "- **Opik Integration**: Tracks experiments and visualizes results\n",
                "\n",
                "## Prerequisites\n",
                "```bash\n",
                "pip install opik openai python-dotenv\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (uncomment if needed)\n",
                "# !pip install opik openai python-dotenv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'opik'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopik\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopik\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m track\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopik\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'opik'"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import opik\n",
                "from opik import track\n",
                "from opik.evaluation import evaluate\n",
                "from opik.evaluation.metrics import base_metric, score_result\n",
                "from openai import OpenAI\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Initialize Opik (this will create a local instance or connect to cloud)\n",
                "opik_client = opik.Opik()\n",
                "\n",
                "# Initialize OpenAI for the judge LLM\n",
                "judge_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
                "\n",
                "print(\"‚úÖ Opik and OpenAI initialized successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Custom LLM Judge Metrics\n",
                "\n",
                "We'll create three custom judges:\n",
                "1. **Financial Accuracy Judge**: Ensures advice is financially sound\n",
                "2. **Tone Judge**: Checks for professional, witty, and frugal tone\n",
                "3. **Helpfulness Judge**: Evaluates if the response addresses the user's need"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FinancialAccuracyJudge(base_metric.BaseMetric):\n",
                "    \"\"\"\n",
                "    Custom LLM judge that evaluates financial accuracy of AI responses.\n",
                "    Scores from 0.0 (incorrect/harmful) to 1.0 (accurate and sound advice).\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, name: str = \"financial_accuracy\"):\n",
                "        super().__init__(name=name)\n",
                "    \n",
                "    def score(self, output: str, context: str = None, **kwargs) -> score_result.ScoreResult:\n",
                "        \"\"\"\n",
                "        Evaluates financial accuracy using GPT-4 as a judge.\n",
                "        \n",
                "        Args:\n",
                "            output: The AI's response to evaluate\n",
                "            context: Optional context about the user's query\n",
                "        \n",
                "        Returns:\n",
                "            ScoreResult with value between 0.0 and 1.0\n",
                "        \"\"\"\n",
                "        \n",
                "        judge_prompt = f\"\"\"You are a financial advisor expert evaluating AI-generated financial advice.\n",
                "\n",
                "Evaluate the following response for financial accuracy:\n",
                "\n",
                "USER QUERY: {context or 'Not provided'}\n",
                "AI RESPONSE: {output}\n",
                "\n",
                "Evaluation Criteria:\n",
                "1. Is the financial advice sound and safe?\n",
                "2. Does it avoid promoting risky behavior (e.g., using credit for cash, overspending)?\n",
                "3. Are calculations or assessments accurate?\n",
                "4. Does it promote responsible financial management?\n",
                "\n",
                "Respond with ONLY a score from 0.0 to 1.0:\n",
                "- 0.0 = Dangerous/incorrect financial advice\n",
                "- 0.5 = Partially correct but room for improvement\n",
                "- 1.0 = Excellent, accurate, and responsible advice\n",
                "\n",
                "Your score:\"\"\"\n",
                "        \n",
                "        response = judge_client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=10\n",
                "        )\n",
                "        \n",
                "        score_text = response.choices[0].message.content.strip()\n",
                "        \n",
                "        try:\n",
                "            score = float(score_text)\n",
                "            score = max(0.0, min(1.0, score))  # Clamp to [0, 1]\n",
                "        except ValueError:\n",
                "            score = 0.5  # Default to neutral if parsing fails\n",
                "        \n",
                "        return score_result.ScoreResult(\n",
                "            value=score,\n",
                "            name=self.name,\n",
                "            reason=f\"Financial accuracy score: {score:.2f}\"\n",
                "        )\n",
                "\n",
                "print(\"‚úÖ FinancialAccuracyJudge defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ToneJudge(base_metric.BaseMetric):\n",
                "    \"\"\"\n",
                "    Custom LLM judge that evaluates tone: professional, witty, and frugal.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, name: str = \"tone_quality\"):\n",
                "        super().__init__(name=name)\n",
                "    \n",
                "    def score(self, output: str, **kwargs) -> score_result.ScoreResult:\n",
                "        judge_prompt = f\"\"\"You are a communication expert evaluating AI response tone.\n",
                "\n",
                "Evaluate this response:\n",
                "{output}\n",
                "\n",
                "Criteria:\n",
                "1. Professional yet approachable\n",
                "2. Witty and engaging (uses metaphors/creative language)\n",
                "3. Frugal mindset (promotes smart financial choices)\n",
                "\n",
                "Score from 0.0 to 1.0:\n",
                "- 0.0 = Dry, boring, or unprofessional\n",
                "- 0.5 = Adequate but lacks personality\n",
                "- 1.0 = Perfect balance of professional, witty, and frugal\n",
                "\n",
                "Your score:\"\"\"\n",
                "        \n",
                "        response = judge_client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=10\n",
                "        )\n",
                "        \n",
                "        score_text = response.choices[0].message.content.strip()\n",
                "        \n",
                "        try:\n",
                "            score = float(score_text)\n",
                "            score = max(0.0, min(1.0, score))\n",
                "        except ValueError:\n",
                "            score = 0.5\n",
                "        \n",
                "        return score_result.ScoreResult(\n",
                "            value=score,\n",
                "            name=self.name,\n",
                "            reason=f\"Tone quality score: {score:.2f}\"\n",
                "        )\n",
                "\n",
                "print(\"‚úÖ ToneJudge defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HelpfulnessJudge(base_metric.BaseMetric):\n",
                "    \"\"\"\n",
                "    Custom LLM judge that evaluates if the response actually helps the user.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, name: str = \"helpfulness\"):\n",
                "        super().__init__(name=name)\n",
                "    \n",
                "    def score(self, output: str, context: str = None, **kwargs) -> score_result.ScoreResult:\n",
                "        judge_prompt = f\"\"\"You are evaluating if an AI response is helpful.\n",
                "\n",
                "USER QUERY: {context or 'Not provided'}\n",
                "AI RESPONSE: {output}\n",
                "\n",
                "Criteria:\n",
                "1. Directly addresses the user's question\n",
                "2. Provides actionable insights or clear answers\n",
                "3. Explains reasoning (not just \"yes\" or \"no\")\n",
                "\n",
                "Score from 0.0 to 1.0:\n",
                "- 0.0 = Unhelpful, off-topic, or confusing\n",
                "- 0.5 = Somewhat helpful but incomplete\n",
                "- 1.0 = Extremely helpful, clear, and actionable\n",
                "\n",
                "Your score:\"\"\"\n",
                "        \n",
                "        response = judge_client.chat.completions.create(\n",
                "            model=\"gpt-4\",\n",
                "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
                "            temperature=0,\n",
                "            max_tokens=10\n",
                "        )\n",
                "        \n",
                "        score_text = response.choices[0].message.content.strip()\n",
                "        \n",
                "        try:\n",
                "            score = float(score_text)\n",
                "            score = max(0.0, min(1.0, score))\n",
                "        except ValueError:\n",
                "            score = 0.5\n",
                "        \n",
                "        return score_result.ScoreResult(\n",
                "            value=score,\n",
                "            name=self.name,\n",
                "            reason=f\"Helpfulness score: {score:.2f}\"\n",
                "        )\n",
                "\n",
                "print(\"‚úÖ HelpfulnessJudge defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Evaluation Dataset\n",
                "\n",
                "We'll create test cases with expected quality benchmarks:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test dataset with queries and expected AI responses\n",
                "evaluation_dataset = [\n",
                "    {\n",
                "        \"input\": \"I want to buy a second-hand car for ‚Ç±180,000 in cash. Can I afford it?\",\n",
                "        \"output\": \"Hold your horses! Your cash and debit wallets currently total ‚Ç±83,450. To buy that ‚Ç±180,000 car, you'd be quite a bit short. Dipping into credit for a cash purchase is a slippery slope to a debt spiral; it's always best to save up for big buys to keep your financial house in order.\",\n",
                "        \"expected_quality\": \"high\"  # We expect high scores across all metrics\n",
                "    },\n",
                "    {\n",
                "        \"input\": \"What's my total balance across all wallets?\",\n",
                "        \"output\": \"Your total financial well-being, a true treasure chest, stands at a robust $163,099! Keep nurturing those funds; every pennyaved is a future freedom earned.\",\n",
                "        \"expected_quality\": \"high\"\n",
                "    },\n",
                "    {\n",
                "        \"input\": \"Should I use my credit card or debit for groceries?\",\n",
                "        \"output\": \"Use your credit card if it offers cashback on groceries, but only if you pay the full balance monthly. Otherwise, it's a bumpy road with high interest. Debit keeps your budget grounded.\",\n",
                "        \"expected_quality\": \"high\"\n",
                "    },\n",
                "    {\n",
                "        \"input\": \"Can I withdraw cash using my credit card?\",\n",
                "        \"output\": \"Go ahead and withdraw cash with your credit card!\",  # BAD ADVICE - should score low\n",
                "        \"expected_quality\": \"low\"  # Expected to fail financial accuracy\n",
                "    }\n",
                "]\n",
                "\n",
                "print(f\"‚úÖ Created evaluation dataset with {len(evaluation_dataset)} test cases\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run LLM Judge Evaluation\n",
                "\n",
                "Now let's evaluate each response using our custom judges:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize judges\n",
                "financial_judge = FinancialAccuracyJudge()\n",
                "tone_judge = ToneJudge()\n",
                "helpfulness_judge = HelpfulnessJudge()\n",
                "\n",
                "# Function to evaluate a single response\n",
                "@track\n",
                "def evaluate_response(input_query: str, output_response: str):\n",
                "    \"\"\"\n",
                "    Evaluates a single AI response using all three judges.\n",
                "    Opik's @track decorator automatically logs this to the Opik platform.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Score with each judge\n",
                "    financial_score = financial_judge.score(output=output_response, context=input_query)\n",
                "    tone_score = tone_judge.score(output=output_response)\n",
                "    helpfulness_score = helpfulness_judge.score(output=output_response, context=input_query)\n",
                "    \n",
                "    return {\n",
                "        \"input\": input_query,\n",
                "        \"output\": output_response,\n",
                "        \"financial_accuracy\": financial_score.value,\n",
                "        \"tone_quality\": tone_score.value,\n",
                "        \"helpfulness\": helpfulness_score.value,\n",
                "        \"average_score\": (financial_score.value + tone_score.value + helpfulness_score.value) / 3\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Evaluation function defined with Opik tracking\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation on all test cases\n",
                "results = []\n",
                "\n",
                "print(\"\\nüöÄ Running LLM Judge Evaluations...\\n\")\n",
                "\n",
                "for i, test_case in enumerate(evaluation_dataset, 1):\n",
                "    print(f\"üìù Test Case {i}/{len(evaluation_dataset)}\")\n",
                "    print(f\"Query: {test_case['input'][:60]}...\")\n",
                "    \n",
                "    result = evaluate_response(\n",
                "        input_query=test_case['input'],\n",
                "        output_response=test_case['output']\n",
                "    )\n",
                "    \n",
                "    result['expected_quality'] = test_case['expected_quality']\n",
                "    results.append(result)\n",
                "    \n",
                "    print(f\"   üìä Financial Accuracy: {result['financial_accuracy']:.2f}\")\n",
                "    print(f\"   üìä Tone Quality: {result['tone_quality']:.2f}\")\n",
                "    print(f\"   üìä Helpfulness: {result['helpfulness']:.2f}\")\n",
                "    print(f\"   ‚≠ê Average Score: {result['average_score']:.2f}\")\n",
                "    print()\n",
                "\n",
                "print(\"‚úÖ Evaluation complete! Results logged to Opik.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Convert to DataFrame for analysis\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "print(\"\\nüìà EVALUATION SUMMARY\\n\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Overall averages\n",
                "print(f\"\\nüéØ Overall Metrics:\")\n",
                "print(f\"   Financial Accuracy: {df['financial_accuracy'].mean():.2f} (¬±{df['financial_accuracy'].std():.2f})\")\n",
                "print(f\"   Tone Quality:       {df['tone_quality'].mean():.2f} (¬±{df['tone_quality'].std():.2f})\")\n",
                "print(f\"   Helpfulness:        {df['helpfulness'].mean():.2f} (¬±{df['helpfulness'].std():.2f})\")\n",
                "print(f\"   Average Score:      {df['average_score'].mean():.2f}\")\n",
                "\n",
                "# Group by expected quality\n",
                "print(f\"\\nüìä By Expected Quality:\")\n",
                "grouped = df.groupby('expected_quality')[['financial_accuracy', 'tone_quality', 'helpfulness', 'average_score']].mean()\n",
                "print(grouped)\n",
                "\n",
                "# Display full results table\n",
                "print(f\"\\nüìã Detailed Results:\")\n",
                "display(df[['input', 'financial_accuracy', 'tone_quality', 'helpfulness', 'average_score', 'expected_quality']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualize Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Create visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# 1. Metric comparison across test cases\n",
                "x = np.arange(len(results))\n",
                "width = 0.25\n",
                "\n",
                "axes[0].bar(x - width, df['financial_accuracy'], width, label='Financial Accuracy', alpha=0.8)\n",
                "axes[0].bar(x, df['tone_quality'], width, label='Tone Quality', alpha=0.8)\n",
                "axes[0].bar(x + width, df['helpfulness'], width, label='Helpfulness', alpha=0.8)\n",
                "\n",
                "axes[0].set_xlabel('Test Case')\n",
                "axes[0].set_ylabel('Score')\n",
                "axes[0].set_title('LLM Judge Scores by Test Case')\n",
                "axes[0].set_xticks(x)\n",
                "axes[0].set_xticklabels([f\"Test {i+1}\" for i in range(len(results))])\n",
                "axes[0].legend()\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "axes[0].set_ylim(0, 1.1)\n",
                "\n",
                "# 2. Average scores by quality expectation\n",
                "quality_groups = df.groupby('expected_quality')['average_score'].mean()\n",
                "colors = ['#ff6b6b' if q == 'low' else '#51cf66' for q in quality_groups.index]\n",
                "\n",
                "axes[1].bar(quality_groups.index, quality_groups.values, color=colors, alpha=0.8)\n",
                "axes[1].set_xlabel('Expected Quality')\n",
                "axes[1].set_ylabel('Average Score')\n",
                "axes[1].set_title('Average Score by Expected Quality')\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "axes[1].set_ylim(0, 1.1)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Visualization complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Using Opik's Built-in Evaluation Framework\n",
                "\n",
                "Opik also provides a higher-level `evaluate()` function for batch evaluations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from opik.evaluation import evaluate\n",
                "\n",
                "# Define the task (wraps your AI application)\n",
                "def wais_wallet_task(input_data):\n",
                "    \"\"\"Simulates calling the Wais Wallet AI Pilot\"\"\"\n",
                "    # In a real scenario, this would call your actual API\n",
                "    # For now, we'll return the pre-recorded responses\n",
                "    for test_case in evaluation_dataset:\n",
                "        if test_case['input'] == input_data['input']:\n",
                "            return {\"output\": test_case['output']}\n",
                "    return {\"output\": \"Unable to process request\"}\n",
                "\n",
                "# Run Opik's evaluate function\n",
                "evaluation_results = evaluate(\n",
                "    dataset=evaluation_dataset,\n",
                "    task=wais_wallet_task,\n",
                "    scoring_metrics=[financial_judge, tone_judge, helpfulness_judge],\n",
                "    experiment_name=\"wais_wallet_llm_judge_v1\"\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Opik evaluation complete!\")\n",
                "print(f\"üìä Results logged to experiment: wais_wallet_llm_judge_v1\")\n",
                "print(f\"üîó View in Opik UI: http://localhost:5000 (if running locally)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Key Takeaways\n",
                "\n",
                "### What We Built:\n",
                "1. **Three Custom LLM Judges**:\n",
                "   - Financial Accuracy Judge\n",
                "   - Tone Quality Judge\n",
                "   - Helpfulness Judge\n",
                "\n",
                "2. **Automated Evaluation Pipeline**:\n",
                "   - Batch evaluation of AI responses\n",
                "   - Opik tracking and logging\n",
                "   - Visualization of results\n",
                "\n",
                "3. **Quality Assurance Framework**:\n",
                "   - Can detect poor financial advice (e.g., Test Case 4)\n",
                "   - Validates tone and helpfulness\n",
                "   - Provides quantitative metrics for A/B testing prompts\n",
                "\n",
                "### Next Steps:\n",
                "- Integrate this into CI/CD for automated testing\n",
                "- Use judge scores to compare different prompt versions\n",
                "- Create alerts when scores drop below thresholds\n",
                "- Expand dataset with real user queries\n",
                "\n",
                "### Benefits:\n",
                "- **Objective Evaluation**: Consistent scoring across experiments\n",
                "- **Scalable**: Can evaluate thousands of responses automatically\n",
                "- **Traceable**: Opik tracks all evaluations for future analysis\n",
                "- **Flexible**: Easy to add new judges for different criteria"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
